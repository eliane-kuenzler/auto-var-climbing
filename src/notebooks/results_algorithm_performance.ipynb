{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the algorithm performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for output of plots\n",
    "output_path = '../../data/output/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the DataFrame file\n",
    "dataframe_path = '../../data/output/plus_algorithm/results_algorithm_performance.pd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame\n",
    "plus_algorithm_results = pd.read_pickle(dataframe_path)\n",
    "print(plus_algorithm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_entries = plus_algorithm_results.shape[0]\n",
    "print(f\"Number of entries in the DataFrame: {num_entries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Update some columns: competition, athlete number, ground truth ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "input_dataframe_path = \"../../data/output/plus_algorithm/results_algorithm_performance.pd\"\n",
    "output_dataframe_path = \"../../data/output/plus_algorithm/updated_results_algorithm_performance.pd\"\n",
    "\n",
    "# Load the DataFrame\n",
    "plus_algorithm_results = pd.read_pickle(input_dataframe_path)\n",
    "\n",
    "# Function to extract details from 'video_name'\n",
    "def extract_details(video_name):\n",
    "    parts = video_name.split(\"_\")\n",
    "    \n",
    "    # Determine competition\n",
    "    competition = \"lenzburg\" if \"lenzburg\" in parts else \"villars\" if \"villars\" in parts else None\n",
    "    \n",
    "    # Determine gender\n",
    "    gender = \"male\" if \"men\" in parts else \"female\" if \"women\" in parts else None\n",
    "\n",
    "    # Extract athlete number (e.g., n11, n103, etc.)\n",
    "    athlete_number_match = re.search(r'n(\\d+)', video_name)\n",
    "    athlete_number = athlete_number_match.group(1) if athlete_number_match else None\n",
    "\n",
    "    # Extract ground truth (last part of the name, could be a number or number with +)\n",
    "    ground_truth_match = re.search(r'(\\d+\\+?|\\d+)', parts[-1])\n",
    "    ground_truth = ground_truth_match.group(1) if ground_truth_match else None\n",
    "\n",
    "    return pd.Series([competition, athlete_number, gender, ground_truth])\n",
    "\n",
    "# Apply extraction function\n",
    "plus_algorithm_results[['competition', 'athlete_number', 'gender', 'ground_truth']] = plus_algorithm_results['video_name'].apply(extract_details)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "plus_algorithm_results.to_pickle(output_dataframe_path)\n",
    "\n",
    "print(\"Updated DataFrame has been saved to:\", output_dataframe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_path = '../../data/output/plus_algorithm/updated_results_algorithm_performance.pd'\n",
    "\n",
    "# Load the DataFrame\n",
    "updated_plus_algorithm_results = pd.read_pickle(dataframe_path)\n",
    "print(updated_plus_algorithm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we add the ground truth plus column\n",
    "# Define file path\n",
    "dataframe_path = '../../data/output/plus_algorithm/updated_results_algorithm_performance.pd'\n",
    "\n",
    "# Load the DataFrame\n",
    "updated_plus_algorithm_results = pd.read_pickle(dataframe_path)\n",
    "\n",
    "# Create 'ground_truth_plus' column (True if \"+\" in ground_truth, False otherwise)\n",
    "updated_plus_algorithm_results[\"ground_truth_plus\"] = updated_plus_algorithm_results[\"ground_truth\"].astype(str).str.contains(\"\\+\")\n",
    "\n",
    "# Save the updated DataFrame\n",
    "updated_plus_algorithm_results.to_pickle(dataframe_path)\n",
    "\n",
    "print(\"Updated DataFrame with 'ground_truth_plus' has been saved to:\", dataframe_path)\n",
    "\n",
    "# Display first few rows to verify\n",
    "print(updated_plus_algorithm_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Gaining insights about the results and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure final_score and ground_truth are treated as strings for proper comparison\n",
    "updated_plus_algorithm_results[\"final_score\"] = updated_plus_algorithm_results[\"final_score\"].astype(str)\n",
    "updated_plus_algorithm_results[\"ground_truth\"] = updated_plus_algorithm_results[\"ground_truth\"].astype(str)\n",
    "\n",
    "# Count number of athletes by gender\n",
    "gender_counts = updated_plus_algorithm_results[\"gender\"].value_counts()\n",
    "\n",
    "# Count number of videos by competition\n",
    "competition_counts = updated_plus_algorithm_results[\"competition\"].value_counts()\n",
    "\n",
    "# Count number of scores with \"+\" and \"noplus\"\n",
    "plus_counts = updated_plus_algorithm_results[\"ground_truth_plus\"].value_counts()\n",
    "\n",
    "# Count how many times final_score matches ground_truth\n",
    "matching_scores = (updated_plus_algorithm_results[\"final_score\"] == updated_plus_algorithm_results[\"ground_truth\"]).sum()\n",
    "\n",
    "# Count total number of rows\n",
    "total_rows = len(updated_plus_algorithm_results)\n",
    "\n",
    "# Count how many scores do NOT match ground_truth\n",
    "non_matching_scores = total_rows - matching_scores\n",
    "\n",
    "# Print Insights\n",
    "print(\"Dataset Insights:\")\n",
    "print(f\"Female Athletes: {gender_counts.get('female', 0)}\")\n",
    "print(f\"Male Athletes: {gender_counts.get('male', 0)}\")\n",
    "print(f\"Videos from Lenzburg: {competition_counts.get('lenzburg', 0)}\")\n",
    "print(f\"Videos from Villars: {competition_counts.get('villars', 0)}\")\n",
    "print(f\"Scores with '+': {plus_counts.get(True, 0)}\")\n",
    "print(f\"Scores with 'noplus': {plus_counts.get(False, 0)}\")\n",
    "print(f\"Final Score matches Ground Truth: {matching_scores}\")\n",
    "print(f\"Final Score does NOT match Ground Truth: {non_matching_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the DataFrame\n",
    "dataframe_path = \"../../data/output/plus_algorithm/updated_results_algorithm_performance.pd\"\n",
    "updated_plus_algorithm_results = pd.read_pickle(dataframe_path)\n",
    "\n",
    "# Capitalize competition names\n",
    "updated_plus_algorithm_results[\"competition\"] = updated_plus_algorithm_results[\"competition\"].str.capitalize()\n",
    "\n",
    "# Count number of videos by competition and gender\n",
    "competition_gender_counts = updated_plus_algorithm_results.groupby([\"competition\", \"gender\"]).size().unstack()\n",
    "\n",
    "# Set Seaborn style for a clean scientific look\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "# competition_gender_counts.plot(kind=\"bar\", stacked=True, width=0.6, color=[\"black\", \"darkgray\"])\n",
    "competition_gender_counts.plot(kind=\"bar\", stacked=True, width=0.6, color=[\"#4E79A7\", \"#76B7B2\"])\n",
    "\n",
    "# Add numbers inside the bars\n",
    "for i, (female, male) in enumerate(zip(competition_gender_counts[\"female\"].fillna(0), \n",
    "                                       competition_gender_counts[\"male\"].fillna(0))):\n",
    "    # Add female count in white (inside the dark bar) if greater than 0\n",
    "    if female > 0:\n",
    "        plt.text(i, female / 2, str(int(female)), ha='center', va='center', fontsize=12, color=\"white\", weight=\"bold\")\n",
    "\n",
    "    # Add male count in black (inside the light bar) if greater than 0\n",
    "    if male > 0:\n",
    "        plt.text(i, female + (male / 2), str(int(male)), ha='center', va='center', fontsize=12, color=\"black\", weight=\"bold\")\n",
    "\n",
    "# competition_gender_counts.plot(kind=\"bar\", stacked=True, width=0.6, color=[\"black\", \"darkgray\"], edgecolor=\"black\")\n",
    "# Customize plot appearance\n",
    "plt.title(\"Number of Videos by Competition and Gender\", fontsize=16)\n",
    "plt.xlabel(\"Competition\", fontsize=14)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.ylabel(\"Number of Videos\", fontsize=14)\n",
    "plt.xticks(rotation=0, fontsize=12)  # Keep competition labels horizontal\n",
    "plt.legend(labels=[\"Female\", \"Male\"], title_fontsize=12, fontsize=12)\n",
    "\n",
    "# Remove the grid\n",
    "plt.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"{output_path}/number_of_videos_by_competition_and_gender_v2.png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Plot saved: number_of_videos_by_competition_and_gender_v2.png\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the DataFrame\n",
    "dataframe_path = \"../../data/output/plus_algorithm/updated_results_algorithm_performance.pd\"\n",
    "updated_plus_algorithm_results = pd.read_pickle(dataframe_path)\n",
    "\n",
    "# Ensure the \"plus\" column is treated as boolean (True/False)\n",
    "updated_plus_algorithm_results[\"ground_truth_plus\"] = updated_plus_algorithm_results[\"ground_truth_plus\"].astype(bool)\n",
    "\n",
    "# Count occurrences of True (Plus) and False (No Plus) and normalize to percentage\n",
    "plus_counts = updated_plus_algorithm_results[\"ground_truth_plus\"].value_counts(normalize=True) * 100  # Convert to percentage\n",
    "\n",
    "# Assign correct labels based on actual order\n",
    "labels = [\"Plus\", \"No Plus\"] if plus_counts.index[0] else [\"No Plus\", \"Plus\"]\n",
    "\n",
    "# Define correct colors: No Plus = Orange, Plus = Teal\n",
    "colors = [\"teal\", \"orange\"] if plus_counts.index[0] else [\"orange\", \"teal\"]\n",
    "\n",
    "# Create a donut chart with percentage labels moved outward\n",
    "plt.figure(figsize=(7, 7))\n",
    "wedges, texts, autotexts = plt.pie(\n",
    "    plus_counts, labels=labels, autopct='%1.1f%%',\n",
    "    colors=colors, startangle=90, wedgeprops={'edgecolor': 'white'},\n",
    "    textprops={'fontsize': 12, 'weight': 'bold', 'color': 'black'}, pctdistance=0.77\n",
    ")\n",
    "\n",
    "# Add a smaller white circle at the center to create the \"donut\" effect\n",
    "centre_circle = plt.Circle((0, 0), 0.55, fc=\"white\")\n",
    "plt.gca().add_artist(centre_circle)\n",
    "\n",
    "plt.title(\"Distribution of Scores with and without a Plus\", fontsize=16, y=0.95)  # Move title closer\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f\"{output_path}/plus_distribution_over_all_videos.png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Plot saved: plus_distribution_over_all_videos.png\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Print the actual counts to verify correctness\n",
    "print(plus_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load the DataFrame\n",
    "dataframe_path = \"../../data/output/plus_algorithm/updated_results_algorithm_performance.pd\"\n",
    "updated_plus_algorithm_results = pd.read_pickle(dataframe_path)\n",
    "\n",
    "# Ensure final_score and ground_truth are treated as strings for comparison\n",
    "updated_plus_algorithm_results[\"final_score\"] = updated_plus_algorithm_results[\"final_score\"].astype(str)\n",
    "updated_plus_algorithm_results[\"ground_truth\"] = updated_plus_algorithm_results[\"ground_truth\"].astype(str)\n",
    "\n",
    "# Create a column for whether the final score matches ground truth\n",
    "updated_plus_algorithm_results[\"match\"] = updated_plus_algorithm_results[\"final_score\"] == updated_plus_algorithm_results[\"ground_truth\"]\n",
    "\n",
    "# Count correct and incorrect identifications by competition and gender\n",
    "match_counts = updated_plus_algorithm_results.groupby([\"competition\", \"gender\", \"match\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Ensure consistent ordering of categories\n",
    "match_counts = match_counts.reindex(columns=[True, False])  # True = Match, False = No Match\n",
    "\n",
    "# Custom x-axis labels with line breaks\n",
    "custom_labels = [\n",
    "    \"Lenzburg\\nWomen's Final\",\n",
    "    \"Villars\\nWomen's Semifinal\",\n",
    "    \"Villars\\nMen's Semifinal\"\n",
    "]\n",
    "\n",
    "# Create stacked bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = match_counts.plot(kind=\"bar\", stacked=True, width=0.6, \n",
    "                       color=[\"#006D77\", \"#83C5BE\"], ax=plt.gca())\n",
    "\n",
    "# Adjust y-axis range\n",
    "plt.ylim(0, 18)\n",
    "\n",
    "# Set custom y-axis ticks\n",
    "custom_yticks = [1, 3, 5, 7, 9, 11, 13, 15]  # No ticks above 15\n",
    "# Customize y-axis labels (increase font size)\n",
    "plt.yticks(custom_yticks, fontsize=12)\n",
    "\n",
    "# Customize x-axis labels\n",
    "plt.xticks(ticks=range(len(custom_labels)), labels=custom_labels, rotation=0, fontsize=12)\n",
    "\n",
    "# Enable only horizontal grid lines with dashed style and transparency\n",
    "plt.grid(visible=False)  # Disable all grids first\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)  # Enable only horizontal grid\n",
    "\n",
    "\n",
    "# Add percentages inside the \"Correctly Identified\" section of the bars\n",
    "for i, (correct, incorrect) in enumerate(zip(match_counts[True], match_counts[False])):\n",
    "    total = correct + incorrect\n",
    "    if total > 0:\n",
    "        percentage = (correct / total) * 100\n",
    "        plt.text(i, correct / 2, f\"{int(percentage)}%\", ha='center', va='center', fontsize=12, weight='bold', color=\"white\")\n",
    "\n",
    "# Customize plot appearance\n",
    "plt.title(\"Score Identification by Competition and Gender\", fontsize=16, y=1.02)\n",
    "# plt.xlabel(\"Competition and Route\", fontsize=14)\n",
    "plt.xlabel(\"\", fontsize=14)\n",
    "plt.ylabel(\"Number of Videos for Scoring\", fontsize=14)\n",
    "plt.legend(labels=[\"Correct Score Output\", \"Incorrect Score Output\"], loc=\"upper left\", title_fontsize=12, fontsize=12)\n",
    "# plt.legend(title=\"Score identified\", labels=[\"Correct\", \"Incorrect\"], loc=\"upper left\", title_fontsize=12, fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f\"{output_path}/score_correctness_over_routes_v2.png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Plot saved: score_correctness_over_routes_v2.png\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the distribution of scores at each competition and the videos that were preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame\n",
    "dataframe_path = '../../data/output/plus_algorithm/updated_results_algorithm_performance.pd'\n",
    "results_analysis = pd.read_pickle(dataframe_path)\n",
    "\n",
    "# Drop NaN values in ground_truth before conversion\n",
    "results_analysis = results_analysis.dropna(subset=[\"ground_truth\"])\n",
    "\n",
    "# Convert ground_truth to integer values (remove '+') for plotting\n",
    "results_analysis[\"ground_truth_int\"] = results_analysis[\"ground_truth\"].astype(str).str.replace(\"+\", \"\", regex=False).astype(int)\n",
    "\n",
    "# Convert athlete_number to numeric for correct sorting\n",
    "results_analysis[\"athlete_number\"] = pd.to_numeric(results_analysis[\"athlete_number\"], errors='coerce')\n",
    "\n",
    "# Identify which ground_truth values contain '+'\n",
    "results_analysis[\"has_plus\"] = results_analysis[\"ground_truth\"].astype(str).str.contains(\"\\+\")\n",
    "\n",
    "# Define competition and gender combinations for plotting with specific score ranges\n",
    "plot_configs = [\n",
    "    (\"villars\", \"female\", 1, 51, \"Score Distribution in Villars Semifinals - Women\"),\n",
    "    (\"villars\", \"male\", 1, 44, \"Score Distribution in Villars Semifinals - Men\"),\n",
    "    (\"lenzburg\", \"female\", 1, 46, \"Score Distribution in Lenzburg Finals - Women\"),\n",
    "]\n",
    "\n",
    "# Generate separate scatter plots for each competition and gender\n",
    "for competition, gender, score_min, score_max, plot_title in plot_configs:\n",
    "    subset = results_analysis[\n",
    "        (results_analysis[\"competition\"] == competition) & \n",
    "        (results_analysis[\"gender\"] == gender)\n",
    "    ].sort_values(\"athlete_number\")  # Ensure athletes are sorted\n",
    "\n",
    "    if not subset.empty:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Create categorical x-axis labels (equal spacing)\n",
    "        subset[\"athlete_index\"] = range(len(subset))  # Creates a sequential index for equal spacing\n",
    "\n",
    "        # Define color mapping for legend\n",
    "        color_map = {True: \"teal\", False: \"orange\"}\n",
    "        point_colors = subset[\"has_plus\"].map(color_map)\n",
    "\n",
    "        # Create scatter plot with equal spacing for athlete numbers\n",
    "        scatter = sns.scatterplot(x=subset[\"athlete_index\"], y=subset[\"ground_truth_int\"], \n",
    "                                  s=100, palette=color_map, hue=subset[\"has_plus\"], legend=True)\n",
    "\n",
    "        # Rename legend labels\n",
    "        legend_labels = {True: \"Plus\", False: \"No Plus\"}\n",
    "        handles, labels = scatter.get_legend_handles_labels()\n",
    "        labels = [legend_labels[eval(label)] for label in labels]  # Rename labels\n",
    "        plt.legend(handles, labels, title=\"Progression\", loc=\"upper right\", title_fontsize=12, fontsize=12)\n",
    "\n",
    "        # Annotate each point below the point\n",
    "        for i, row in subset.iterrows():\n",
    "            plt.text(row[\"athlete_index\"], row[\"ground_truth_int\"] - 1.5, str(row[\"ground_truth\"]),\n",
    "                     horizontalalignment='center', verticalalignment='top', fontsize=12, color='black')\n",
    "\n",
    "        # Set y-axis limits to match the given competition range\n",
    "        plt.ylim(score_min, score_max)\n",
    "\n",
    "        # Ensure y-axis uses steps of 5\n",
    "        plt.yticks(np.arange(score_min, score_max + 1, 5))\n",
    "\n",
    "        # Ensure all athlete numbers are equally spaced\n",
    "        plt.xticks(subset[\"athlete_index\"], labels=subset[\"athlete_number\"].astype(str), fontsize=12)\n",
    "\n",
    "        # Improve layout\n",
    "        plt.title(plot_title, fontsize=16)\n",
    "        plt.xlabel(\"Athlete Number\", fontsize=14)\n",
    "        plt.ylabel(\"Score\", fontsize=14)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.25)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # **Save the plot**\n",
    "        filename = f\"{output_path}/scoring_distribution_{competition}_{gender}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Plot saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Load the DataFrame\n",
    "dataframe_path = '../../data/output/plus_algorithm/updated_results_algorithm_performance.pd'\n",
    "results_analysis = pd.read_pickle(dataframe_path)\n",
    "\n",
    "# Drop NaN values in ground_truth before conversion\n",
    "results_analysis = results_analysis.dropna(subset=[\"ground_truth\"])\n",
    "\n",
    "# Convert ground_truth to integer values (remove '+') for plotting\n",
    "results_analysis[\"ground_truth_int\"] = results_analysis[\"ground_truth\"].astype(str).str.replace(\"+\", \"\", regex=False).astype(int)\n",
    "\n",
    "# Convert athlete_number to numeric for correct sorting\n",
    "results_analysis[\"athlete_number\"] = pd.to_numeric(results_analysis[\"athlete_number\"], errors='coerce')\n",
    "\n",
    "# Identify which ground_truth values contain '+'\n",
    "results_analysis[\"has_plus\"] = results_analysis[\"ground_truth\"].astype(str).str.contains(\"\\+\")\n",
    "\n",
    "# Define competition and gender combinations for plotting with specific score ranges\n",
    "plot_configs = [\n",
    "    (\"villars\", \"female\", 1, 51, \"Score Distribution in Villars Women's Semifinals\"),\n",
    "    (\"villars\", \"male\", 1, 44, \"Score Distribution in Villars Men's Semifinals\"),\n",
    "    (\"lenzburg\", \"female\", 1, 46, \"Score Distribution in Lenzburg Women's Finals\"),\n",
    "]\n",
    "\n",
    "# Generate separate bar plots for each competition and gender\n",
    "for competition, gender, score_min, score_max, plot_title in plot_configs:\n",
    "    subset = results_analysis[\n",
    "        (results_analysis[\"competition\"] == competition) & \n",
    "        (results_analysis[\"gender\"] == gender)\n",
    "    ].sort_values(\"ground_truth_int\", ascending=True)  # Sort by score, lowest to highest\n",
    "\n",
    "    if not subset.empty:\n",
    "        num_bars = len(subset)\n",
    "\n",
    "        # **Ensure consistent spacing & height across all plots**\n",
    "        bar_height = 0.7  # Fixed height for uniform bars\n",
    "        num_bars = len(subset)\n",
    "        fig_height = max(5, num_bars * 0.5)  # Scale dynamically but ensure a minimum height\n",
    "\n",
    "        # Create figure with dynamic height\n",
    "        plt.figure(figsize=(10, fig_height))\n",
    "\n",
    "        # Define color mapping\n",
    "        color_map = {True: \"teal\", False: \"orange\"}\n",
    "        bar_colors = subset[\"has_plus\"].map(color_map)\n",
    "\n",
    "        # Create horizontal bar plot with fixed bar height\n",
    "        bars = plt.barh(subset[\"athlete_number\"].astype(str), subset[\"ground_truth_int\"], \n",
    "                        color=bar_colors, height=bar_height)\n",
    "\n",
    "        # Explicitly set Y-Ticks to maintain even spacing\n",
    "        plt.gca().set_yticks(range(len(subset)))\n",
    "        plt.gca().set_yticklabels(subset[\"athlete_number\"].astype(str), fontsize=12)\n",
    "\n",
    "        # Annotate each bar with the actual ground_truth (with '+')\n",
    "        for bar, (score, plus) in zip(bars, zip(subset[\"ground_truth\"], subset[\"has_plus\"])):\n",
    "            plt.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, str(score),\n",
    "                     verticalalignment='center', fontsize=12, color=\"black\")\n",
    "\n",
    "        # Set x-axis limits to match the given competition range\n",
    "        plt.xlim(score_min, score_max + 2)  # Adding small padding for text visibility\n",
    "\n",
    "        # Ensure x-axis uses steps of 5\n",
    "        plt.xticks(np.arange(score_min, score_max + 1, 5), fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "\n",
    "        # Customize grid and labels\n",
    "        plt.title(plot_title, fontsize=16)\n",
    "        plt.xlabel(\"Score\", fontsize=14)\n",
    "        plt.ylabel(\"Bib Number (Athlete)\", fontsize=14)\n",
    "\n",
    "        # Create legend manually\n",
    "        legend_patches = [Patch(color=\"teal\", label=\"Plus\"), Patch(color=\"orange\", label=\"No Plus\")]\n",
    "        plt.legend(handles=legend_patches, loc=\"lower right\", fontsize=12)\n",
    "\n",
    "        # Improve layout\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # **Save the plot**\n",
    "        filename = f\"{output_path}/scoring_distribution_barplot_{competition}_{gender}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Plot saved: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at how the algorithm performance progressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the data for algorithm progression\n",
    "data = {\n",
    "    \"Version\": [\n",
    "        \"First Version: Baseline\",\n",
    "        \"Fall Detection Improvement\",\n",
    "        \"Duohold Implementation\"\n",
    "    ],\n",
    "    \"Correctly Identified\": [15, 20, 27],\n",
    "    \"Total\": [30, 30, 30]  # Total attempts per version\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "progression_df = pd.DataFrame(data)\n",
    "progression_df[\"Accuracy (%)\"] = (progression_df[\"Correctly Identified\"] / progression_df[\"Total\"]) * 100\n",
    "\n",
    "# Define x-axis positions with better alignment\n",
    "x_positions = np.arange(len(progression_df))\n",
    "\n",
    "# Create line plot\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(x_positions, progression_df[\"Accuracy (%)\"], marker=\"o\", linestyle=\"-\", \n",
    "         color=\"teal\", linewidth=2, markersize=8)\n",
    "\n",
    "# Add percentage labels next to each point\n",
    "for i, percentage in enumerate(progression_df[\"Accuracy (%)\"]):\n",
    "    plt.text(x_positions[i], percentage + 3, f\"{int(percentage)}%\",  # Increased spacing from +2 to +3\n",
    "             ha='center', fontsize=12, weight='bold', color=\"black\")\n",
    "\n",
    "# Adjust x-axis limits to add spacing on both sides\n",
    "plt.xlim(-0.5, len(progression_df) - 0.5)  # Adds 0.5 space on both sides\n",
    "\n",
    "# Adjust x-axis labels alignment\n",
    "plt.xticks(x_positions, progression_df[\"Version\"], fontsize=12, ha='center')\n",
    "\n",
    "# Set y-axis range from 0 to 100%\n",
    "plt.ylim(0, 105)\n",
    "plt.yticks(np.arange(0, 101, 20), fontsize=12)\n",
    "\n",
    "# Customize grid (horizontal dashed lines only)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Customize plot appearance\n",
    "plt.title(\"Algorithm Performance Over Versions\", fontsize=16, y=1.02)\n",
    "plt.xlabel(\"\", fontsize=14)\n",
    "plt.ylabel(\"Accuracy (%)\", fontsize=14)\n",
    "\n",
    "# Improve layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f\"{output_path}/algorithm_progression_accuracy_lineplot.png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Plot saved: algorithm_progression_accuracy_lineplot.png\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Confusion Matrix for detecting a plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the confusion matrix (True Positives, False Positives, False Negatives, True Negatives).\n",
    "\n",
    "Calculate performance metrics:\n",
    "- Precision = TP / (TP + FP) → How many predicted pluses are actually correct?\n",
    "- Recall (Sensitivity) = TP / (TP + FN) → How many actual pluses were identified?\n",
    "- F1-score = 2 * (Precision * Recall) / (Precision + Recall) → Harmonic mean of precision and recall\n",
    "- Accuracy = (TP + TN) / (Total Samples) → Overall correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Load the DataFrame\n",
    "dataframe_path = \"../../data/output/plus_algorithm/updated_results_algorithm_performance.pd\"\n",
    "updated_plus_algorithm_results = pd.read_pickle(dataframe_path)\n",
    "\n",
    "# Convert 'plus' and 'ground_truth_plus' to boolean values (if not already)\n",
    "updated_plus_algorithm_results[\"plus\"] = updated_plus_algorithm_results[\"plus\"].astype(bool)\n",
    "updated_plus_algorithm_results[\"ground_truth_plus\"] = updated_plus_algorithm_results[\"ground_truth_plus\"].astype(bool)\n",
    "\n",
    "# Compute Confusion Matrix\n",
    "cm = confusion_matrix(updated_plus_algorithm_results[\"ground_truth_plus\"], updated_plus_algorithm_results[\"plus\"])\n",
    "\n",
    "# Extract values\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "# Calculate Performance Metrics\n",
    "precision = precision_score(updated_plus_algorithm_results[\"ground_truth_plus\"], updated_plus_algorithm_results[\"plus\"])\n",
    "recall = recall_score(updated_plus_algorithm_results[\"ground_truth_plus\"], updated_plus_algorithm_results[\"plus\"])\n",
    "f1 = f1_score(updated_plus_algorithm_results[\"ground_truth_plus\"], updated_plus_algorithm_results[\"plus\"])\n",
    "accuracy = accuracy_score(updated_plus_algorithm_results[\"ground_truth_plus\"], updated_plus_algorithm_results[\"plus\"])\n",
    "\n",
    "# Print Metrics\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall (Sensitivity): {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Plot Confusion Matrix with \"crest\" color palette\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=sns.light_palette(\"teal\", as_cmap=True), \n",
    "            xticklabels=[\"No Plus Pred\", \"Plus Pred\"],\n",
    "            yticklabels=[\"No Plus True\", \"Plus True\"],\n",
    "            annot_kws={\"size\": 16})  # Bigger numbers inside plot\n",
    "\n",
    "# Customize labels\n",
    "plt.xlabel(\"Predicted Label\", fontsize=13, fontweight=\"bold\")\n",
    "plt.ylabel(\"True Label\", fontsize=13, fontweight=\"bold\")\n",
    "plt.title(\"Confusion Matrix for Plus Detection\", fontsize=15)\n",
    "\n",
    "# Customize tick labels\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f\"{output_path}/confusion_matrix_plus.png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Plot saved: confusion_matrix_plus.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making some tabels to have an overview of the data and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from IPython.display import display\n",
    "\n",
    "# Load the DataFrame\n",
    "dataframe_path = \"../../data/output/plus_algorithm/updated_results_algorithm_performance.pd\"\n",
    "updated_plus_algorithm_results = pd.read_pickle(dataframe_path)\n",
    "\n",
    "# Ensure 'plus' and 'ground_truth_plus' are boolean\n",
    "updated_plus_algorithm_results[\"plus\"] = updated_plus_algorithm_results[\"plus\"].astype(bool)\n",
    "updated_plus_algorithm_results[\"ground_truth_plus\"] = updated_plus_algorithm_results[\"ground_truth_plus\"].astype(bool)\n",
    "\n",
    "# Define routes\n",
    "routes = [\n",
    "    (\"lenzburg\", \"female\", \"Women's Final Route\"),\n",
    "    (\"villars\", \"female\", \"Women's Semifinal Route\"),\n",
    "    (\"villars\", \"male\", \"Men's Semifinal Route\"),\n",
    "]\n",
    "\n",
    "# **Manually provided Score Identification Accuracy values**\n",
    "score_identification_accuracy = [\"100%\", \"90%\", \"86%\"]\n",
    "\n",
    "# Store metrics for each route\n",
    "table_data = []\n",
    "for idx, (competition, gender, route_name) in enumerate(routes):\n",
    "    subset = updated_plus_algorithm_results[\n",
    "        (updated_plus_algorithm_results[\"competition\"] == competition) &\n",
    "        (updated_plus_algorithm_results[\"gender\"] == gender)\n",
    "    ]\n",
    "    \n",
    "    # Number of Videos\n",
    "    num_videos = len(subset)\n",
    "\n",
    "    # Plus Detection Metrics\n",
    "    num_pluses = subset[\"ground_truth_plus\"].sum()  # True pluses\n",
    "    detected_pluses = subset[\"plus\"].sum()  # Predicted pluses\n",
    "    \n",
    "    precision = precision_score(subset[\"ground_truth_plus\"], subset[\"plus\"], zero_division=0) * 100 if num_videos > 0 else 0\n",
    "    recall = recall_score(subset[\"ground_truth_plus\"], subset[\"plus\"], zero_division=0) * 100 if num_videos > 0 else 0\n",
    "    f1 = f1_score(subset[\"ground_truth_plus\"], subset[\"plus\"], zero_division=0) * 100 if num_videos > 0 else 0\n",
    "    \n",
    "    # Plus Accuracy (how many actual pluses were identified correctly)\n",
    "    plus_accuracy = ((subset[\"plus\"] & subset[\"ground_truth_plus\"]).sum() / num_pluses * 100) if num_pluses > 0 else 0\n",
    "    \n",
    "    # Append data\n",
    "    table_data.append([\n",
    "        num_videos, \n",
    "        score_identification_accuracy[idx],  # Manually provided accuracy\n",
    "        num_pluses, \n",
    "        detected_pluses, \n",
    "        f\"{precision:.0f}%\", \n",
    "        f\"{recall:.0f}%\", \n",
    "        f\"{f1:.0f}%\", \n",
    "        f\"{plus_accuracy:.0f}%\"  # Plus Accuracy\n",
    "    ])\n",
    "\n",
    "# Convert to DataFrame\n",
    "columns = [\n",
    "    \"Number of Videos\", \n",
    "    \"Score Identification Accuracy\", \n",
    "    \"Number of Pluses\", \n",
    "    \"Detected Pluses\", \n",
    "    \"Precision\", \n",
    "    \"Recall\", \n",
    "    \"F1-score\", \n",
    "    \"Plus Accuracy\"\n",
    "]\n",
    "table_df = pd.DataFrame(table_data, columns=columns)\n",
    "\n",
    "# **Create MultiIndex DataFrame for column grouping (Lenzburg & Villars)**\n",
    "columns = pd.MultiIndex.from_tuples([\n",
    "    (\"Lenzburg\", \"Women's Final Route\"), \n",
    "    (\"Villars\", \"Women's Semifinal Route\"), \n",
    "    (\"Villars\", \"Men's Semifinal Route\")\n",
    "])\n",
    "table_df = table_df.T  # Transpose for structured display\n",
    "table_df.columns = columns  # Assign MultiIndex columns\n",
    "\n",
    "# **Row Grouping: \"Overall\" and \"Plus\" sections**\n",
    "row_labels = [\n",
    "    (\"Overall\", \"Number of Videos\"),\n",
    "    (\"Overall\", \"Score Identification Accuracy\"),\n",
    "    (\"Plus\", \"Number of Pluses\"),\n",
    "    (\"Plus\", \"Detected Pluses\"),\n",
    "    (\"Plus\", \"Precision\"),\n",
    "    (\"Plus\", \"Recall\"),\n",
    "    (\"Plus\", \"F1-score\"),\n",
    "    (\"Plus\", \"Plus Accuracy\"),\n",
    "]\n",
    "\n",
    "# Assign new MultiIndex for rows\n",
    "table_df.index = pd.MultiIndex.from_tuples(row_labels)\n",
    "\n",
    "# **Style the table to center-align headers and values, no background colors**\n",
    "styled_table = table_df.style.set_table_styles([\n",
    "    {\"selector\": \"th\", \"props\": [(\"text-align\", \"center\"), (\"border-bottom\", \"1px solid black\")]}  # **Add thin black line**\n",
    "]).set_properties(**{\"text-align\": \"center\"})\n",
    "# Style the table to center-align headers and values, with border lines\n",
    "styled_table = table_df.style.set_table_styles([\n",
    "    {\"selector\": \"th\", \"props\": [(\"text-align\", \"center\"), (\"border-bottom\", \"1px solid black\")]},  # Header line\n",
    "    {\"selector\": \"td\", \"props\": [(\"text-align\", \"center\"), (\"border-bottom\", \"1px solid black\")]}  # Row line\n",
    "]).set_properties(**{\"text-align\": \"center\"})\n",
    "\n",
    "# Display the styled table\n",
    "display(styled_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Define MultiIndex columns\n",
    "columns = pd.MultiIndex.from_tuples([\n",
    "    (\"Lenzburg\", \"Women's Finale Route\"),\n",
    "    (\"Villars\", \"Women's Semifinal Route\"),\n",
    "    (\"Villars\", \"Men's Semifinal Route\")\n",
    "])\n",
    "\n",
    "# Define the table data\n",
    "data = [\n",
    "    [\"5\", \"10\", \"15\"],    # Number of Videos\n",
    "    [\"100%\", \"90%\", \"86%\"] # Accuracy\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "table_df = pd.DataFrame(data, columns=columns, index=[\"Number of Videos\", \"Accuracy\"])\n",
    "\n",
    "# Style the DataFrame to center-align the headers\n",
    "styled_table = table_df.style.set_table_styles([\n",
    "    {\"selector\": \"th\", \"props\": [(\"text-align\", \"center\")]}\n",
    "]).set_properties(**{\"text-align\": \"center\"})\n",
    "\n",
    "# Display the styled table\n",
    "display(styled_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store metrics for each route (Plus-specific metrics)\n",
    "table_data_plus = []\n",
    "for idx, (competition, gender, route_name) in enumerate(routes):\n",
    "    subset = updated_plus_algorithm_results[\n",
    "        (updated_plus_algorithm_results[\"competition\"] == competition) &\n",
    "        (updated_plus_algorithm_results[\"gender\"] == gender)\n",
    "    ]\n",
    "    \n",
    "    # Plus Detection Metrics\n",
    "    num_pluses = subset[\"ground_truth_plus\"].sum()  # True pluses\n",
    "    detected_pluses = subset[\"plus\"].sum()  # Predicted pluses\n",
    "    \n",
    "    precision = precision_score(subset[\"ground_truth_plus\"], subset[\"plus\"], zero_division=0) * 100 if num_pluses > 0 else 0\n",
    "    recall = recall_score(subset[\"ground_truth_plus\"], subset[\"plus\"], zero_division=0) * 100 if num_pluses > 0 else 0\n",
    "    f1 = f1_score(subset[\"ground_truth_plus\"], subset[\"plus\"], zero_division=0) * 100 if num_pluses > 0 else 0\n",
    "    \n",
    "    # Plus Accuracy (how many actual pluses were identified correctly)\n",
    "    plus_accuracy = ((subset[\"plus\"] & subset[\"ground_truth_plus\"]).sum() / num_pluses * 100) if num_pluses > 0 else 0\n",
    "    \n",
    "    # Append data\n",
    "    table_data_plus.append([\n",
    "        num_pluses, \n",
    "        detected_pluses, \n",
    "        f\"{precision:.0f}%\", \n",
    "        f\"{recall:.0f}%\", \n",
    "        f\"{f1:.0f}%\", \n",
    "        f\"{plus_accuracy:.0f}%\"  # Plus Accuracy\n",
    "    ])\n",
    "\n",
    "# Convert to DataFrame for the Plus table\n",
    "columns_plus = [\n",
    "    \"Number of Pluses\", \n",
    "    \"Detected Pluses\", \n",
    "    \"Precision\", \n",
    "    \"Recall\", \n",
    "    \"F1-score\", \n",
    "    \"Plus Accuracy\"\n",
    "]\n",
    "table_df_plus = pd.DataFrame(table_data_plus, columns=columns_plus)\n",
    "\n",
    "# Define MultiIndex columns\n",
    "columns = pd.MultiIndex.from_tuples([\n",
    "    (\"Lenzburg\", \"Women's Finale Route\"),\n",
    "    (\"Villars\", \"Women's Semifinal Route\"),\n",
    "    (\"Villars\", \"Men's Semifinal Route\")\n",
    "])\n",
    "\n",
    "table_df_plus = table_df_plus.T  # Transpose for structured display\n",
    "table_df_plus.columns = columns  # Assign MultiIndex columns\n",
    "\n",
    "# **Style the table to center-align the headers and values**\n",
    "styled_table_plus = table_df_plus.style.set_table_styles([\n",
    "    {\"selector\": \"th\", \"props\": [(\"text-align\", \"center\")]}  # Center align headers\n",
    "]).set_properties(**{\"text-align\": \"center\"})\n",
    "\n",
    "# Display the styled Plus table\n",
    "display(styled_table_plus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
